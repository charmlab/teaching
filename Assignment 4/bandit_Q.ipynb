{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Bandit Definition"
      ],
      "metadata": {
        "id": "6_-hdD8Q4Rlt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKqUbt1J4HyP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MultiArmedBandit:\n",
        "    \"\"\"A simple multi-armed bandit environment.\"\"\"\n",
        "    def __init__(self, k=10):\n",
        "        self.k = k\n",
        "        # True means are drawn from a standard normal distribution\n",
        "        self.true_means = np.random.normal(0, 1, k)\n",
        "        self.best_arm = np.argmax(self.true_means)\n",
        "\n",
        "    def pull(self, arm):\n",
        "        \"\"\"Pulls an arm and returns a reward from a normal distribution\n",
        "        with the arm's true mean and variance 1.\"\"\"\n",
        "        return np.random.normal(self.true_means[arm], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent Implementation\n"
      ],
      "metadata": {
        "id": "jOVCMJdn4aS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EpsilonGreedyAgent:\n",
        "    \"\"\"An agent that follows the epsilon-greedy strategy.\"\"\"\n",
        "    def __init__(self, k, epsilon=0.1):\n",
        "        self.k = k\n",
        "        self.epsilon = epsilon\n",
        "        self.counts = np.zeros(k, dtype=int)  # Number of times each arm was pulled\n",
        "        self.values = np.zeros(k, dtype=float) # Estimated value of each arm\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an arm using the epsilon-greedy strategy.\"\"\"\n",
        "        # TODO: With probability self.epsilon, select a random arm (explore).\n",
        "        # Otherwise, select the arm with the current highest estimated value (exploit).\n",
        "        pass # REMOVE THIS LINE AND IMPLEMENT\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        \"\"\"Updates the estimated value of the pulled arm.\"\"\"\n",
        "        self.counts[arm] += 1\n",
        "        n = self.counts[arm]\n",
        "        # TODO: Update self.values[arm] using the incremental sample-average formula:\n",
        "        # NewEstimate = OldEstimate + (1/n) * (Reward - OldEstimate)\n",
        "        pass # REMOVE THIS LINE AND IMPLEMENT\n",
        "\n",
        "class EpsilonGreedyDecayAgent(EpsilonGreedyAgent):\n",
        "    \"\"\"An agent that uses a decaying epsilon.\"\"\"\n",
        "    def __init__(self, k):\n",
        "        super().__init__(k)\n",
        "        self.t = 0 # Timestep counter\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an action with an epsilon that decays over time.\"\"\"\n",
        "        self.t += 1\n",
        "        # TODO: Calculate the decaying epsilon for the current timestep t.\n",
        "\n",
        "        # Then, use the same exploration/exploitation logic as the parent class.\n",
        "        pass # REMOVE THIS LINE AND IMPLEMENT\n",
        "\n",
        "class UCB1Agent:\n",
        "    \"\"\"An agent that follows the Upper Confidence Bound (UCB1) strategy.\"\"\"\n",
        "    def __init__(self, k, c=2.0):\n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.counts = np.zeros(k, dtype=int)\n",
        "        self.values = np.zeros(k, dtype=float)\n",
        "        self.t = 0 # Timestep counter\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an arm using the UCB1 formula.\"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        # TODO: First, ensure each arm is played at least once.\n",
        "        # Check if any arm has a count of 0. If so, return that arm.\n",
        "\n",
        "        # TODO: If all arms have been played once, calculate the UCB1 value for each arm.\n",
        "\n",
        "        # TODO: Return the arm with the highest UCB value.\n",
        "        pass # REMOVE THIS LINE AND IMPLEMENT\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        \"\"\"Updates the estimated value of the pulled arm.\"\"\"\n",
        "        self.counts[arm] += 1\n",
        "        n = self.counts[arm]\n",
        "        # TODO: Update self.values[arm] using the incremental sample-average formula.\n",
        "        # (This is the same as in the EpsilonGreedyAgent)\n",
        "        pass # REMOVE THIS LINE AND IMPLEMENT\n",
        "\n"
      ],
      "metadata": {
        "id": "oDjBQ1e24QBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment Runner and Plotting"
      ],
      "metadata": {
        "id": "DfJNfKjA4pg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(agent_class, k=10, runs=2000, steps=1000, **agent_kwargs):\n",
        "    \"\"\"Runs the bandit experiment for a given agent class.\"\"\"\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "    for run in range(runs):\n",
        "        if (run + 1) % 200 == 0:\n",
        "            print(f\"Running {agent_class.__name__}: Run {run+1}/{runs}\")\n",
        "        bandit = MultiArmedBandit(k=k)\n",
        "        agent = agent_class(k=k, **agent_kwargs)\n",
        "        for t in range(steps):\n",
        "            arm = agent.select_action()\n",
        "            reward = bandit.pull(arm)\n",
        "            agent.update(arm, reward)\n",
        "            all_rewards[run, t] = reward\n",
        "    return all_rewards.mean(axis=0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvZTZPKY4c03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run and plot"
      ],
      "metadata": {
        "id": "uE6iOXJdeo0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiments for all agents\n",
        "r_ucb = run_experiment(UCB1Agent, c=2.0, steps=10000)\n",
        "r_fixed = run_experiment(EpsilonGreedyAgent, epsilon=0.1, steps=10000)\n",
        "r_decay = run_experiment(EpsilonGreedyDecayAgent, steps=10000)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(r_ucb, label='UCB1 (c=2)')\n",
        "plt.plot(r_fixed, label='Epsilon-Greedy (ε=0.1)')\n",
        "plt.plot(r_decay, label='Epsilon-Greedy (Decaying ε=1/t)')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Comparison of Multi-Armed Bandit Algorithms')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BEoBBMAa8wld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlsUqkmm5Isj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}